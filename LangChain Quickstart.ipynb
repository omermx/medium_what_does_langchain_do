{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNfiI/OKfZp22rD52N17woK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Shi4EUksqu3_"},"outputs":[],"source":["# First install LangChain\n","!pip install langchain\n"]},{"cell_type":"code","source":["# Environment setup\n","\n","# Install OpenAI model APIs\n","!pip install OpenAI\n","\n","# Store your OpenAI & SerpApi API keys in local variables, you could also store them as enviroment variables\n","OPENAI_API_KEY=\"YOUR_OPEN_API_KEY\"\n","SERP_API_KEY = \"YOUR_SERP_API_KEY\""],"metadata":{"id":"3OajkVrtrAnA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LLMs\n","\n","from langchain.llms import OpenAI\n","\n","# Create and initialize an OpenAI model wrapper, pass in your OpenAI APi key as a parameter.\n","# In this case we want our outputs to be MORE random, so we will assign a HIGH temperature value.\n","llm = OpenAI(openai_api_key=OPENAI_API_KEY,temperature=0.9)\n"],"metadata":{"id":"uhsUnxKWsjyQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now we can pass in text and get some predictions\n","llm.predict(\"What would be a good company name for a company that makes interesting widgets?\")"],"metadata":{"id":"o-GEmnDQtIui"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chat models\n","\n","from langchain.chat_models import ChatOpenAI\n","from langchain.schema import (\n","    AIMessage,\n","    HumanMessage,\n","    SystemMessage\n",")\n","\n","chat = ChatOpenAI(openai_api_key=OPENAI_API_KEY,temperature=0)\n","# There are several types of messages currently supported by LangChain, for example AIMessage, HumanMessage, and SystemMessage.\n","chat.predict_messages([HumanMessage(content=\"Translate this sentence from English to Spanish. Language models are cool!\")])"],"metadata":{"id":"65-ynMeZuqBN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prompt templates\n","\n","from langchain.prompts import PromptTemplate\n","\n","# In our earlier example, we asked the LLM to come up with a company name. Suppose now, we wanted to create a generic prompt template\n","# that could be used as part of an LLM application to come with a company name for whatever product a user chooses.\n","# We can easily do this by creating a reusable prompt template for that purpose:\n","prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n","prompt.format(product=\"interesting widgets\")"],"metadata":{"id":"Q_XTiXix89h4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chains\n","from langchain import LLMChain\n","from langchain.chat_models import ChatOpenAI\n","from langchain.prompts.chat import (\n","    ChatPromptTemplate,\n","    SystemMessagePromptTemplate,\n","    HumanMessagePromptTemplate,\n",")\n","\n","chat = ChatOpenAI(openai_api_key=OPENAI_API_KEY,temperature=0)\n","# Define a tamplate for the base prompt\n","template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n","system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n","human_template = \"{text}\"\n","human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n","chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n","# Chains provide way to link (or chain) together multiple building block, like models, prompts and even other chains!\n","chain = LLMChain(llm=chat, prompt=chat_prompt)\n","# Run the chain that passes values to the placholders in each prompt\n","chain.run(input_language=\"English\", output_language=\"Spanish\", text=\"Language models are cool!\")"],"metadata":{"id":"XLlXtxR7A7Bi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Agents\n","\n","# To load an agent we will need select a(n):\n","# 1. LLM/Chat model: The underlying language model powering the agent\n","# 2. Tool(s): Some function that perfoms a duty. This could be things like: Google Search, Databse lookup, IFTTT webhooks and other chains. For\n","# the complete list see the  Tools documentation\n","# 3. Agent name: This is a text string that refers to a supported agent class. We're going to use one of the standard supported agents, but if you want to dig deeper,\n","# It is possible to implement custom agents for your purposes\n","\n","# For this example we're going to use an Agent with Chat model that will leverage the Google Search results API to query a search engine.\n","\n","# install the SerpAPI Python package:\n","!pip install google-search-results\n"],"metadata":{"id":"rR58FtosO96q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.agents import load_tools\n","from langchain.agents import initialize_agent\n","from langchain.agents import AgentType\n","from langchain.chat_models import ChatOpenAI\n","from langchain.llms import OpenAI\n","\n","# First, let's load the language model we're going to use to control the agent.\n","chat = ChatOpenAI(openai_api_key=OPENAI_API_KEY,temperature=0)\n","\n","# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n","llm = OpenAI(openai_api_key=OPENAI_API_KEY,temperature=0)\n","tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm,serpapi_api_key=SERP_API_KEY)\n","\n","# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n","agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n","\n","# Now let's test it out!\n","agent.run(\"Who is the founder of OpenAI? What is the latest news headline about him?\")"],"metadata":{"id":"aKHPJH5ZU6sU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Memory\n","\n","# The memory module in LangChain provides a way to maintain application state. There are a number of built-in memory systems.\n","# The most simple of these is a buffer memory which just adds the last few inputs or outputs to the beginning of the current input - as shown in the example below:\n","\n","from langchain import OpenAI, ConversationChain\n","\n","llm = OpenAI(openai_api_key=OPENAI_API_KEY,temperature=0)\n","conversation = ConversationChain(llm=llm, verbose=True)\n","\n","conversation.run(\"Hi there!\")"],"metadata":{"id":"9ouxXCmDXGz5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Running the example again, you will see the model contains the input and output from the intial run.\n","conversation.run(\"I'm doing well! My name is Omer!\")"],"metadata":{"id":"FriyDZAFZeGI"},"execution_count":null,"outputs":[]}]}